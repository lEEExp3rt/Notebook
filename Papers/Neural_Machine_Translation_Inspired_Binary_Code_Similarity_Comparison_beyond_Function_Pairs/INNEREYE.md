---
tags:
  - Papers
icon: TiNotes
---

# Neural Machine Translation Inspired Binary Code Similarity Comparison beyond Function Pairs

---

## 0. Abstract

二进制代码分析允许在不接触源代码的情况下进行代码分析

注意到二进制代码经过反汇编后得到对应的汇编语言，这与自然语言处理(*Natural Language Process*)有许多相同之处，如语义提取、分类、文本比较等，因此研究者借鉴了NLP的想法，用于解决二进制代码分析的问题

二进制代码分析主要内容为比较两份二进制代码的相似性，主要体现在以下两个方面：

1. **对于给定的一对来自不同指令集架构下的基本块，判断它们的语义是否相同**
2. **对于给定的一对来自不同指令集架构下的代码片段，判断其中一段代码是否包含在另一段代码中**

以上两个问题的解决有诸多应用前景，如代码的跨架构分析、代码脆弱性发现和代码抄袭的判定等

现阶段针对问题1的解决方案无法兼顾效率与准确性，而神经机器翻译(*Neural Machine Translation*)技术是一种用于解决来自不同自然语言的文本的处理方法，以往的研究者把NMT方法应用于解决问题1，把指令当做单词，把基本块当做语句，通过结合深度学习的方法能够获得很高的效率与准确性

而许多现有的方法已经能够判定两段代码是否等效，但是这属于等效问题，与问题2的包含问题不同，因此解决跨架构代码的包含问题更具挑战

基于此，研究者借鉴了NMT解决问题1的方法，提出了一种新的原型**INNEREYE**，用于解决比问题1更加复杂的问题2，并且通过评估，研究者的模型INNEREYE在效率、准确性和可扩展性方面都优于现有方法

本文还展示了如何将NLP思想和技术应用于大规模二进制代码分析

---

## 1. Introduction

### 1.1 Importance

当今世界上有越来越多的设备正在投入使用，它们

- 来自不同的处理器架构
- 互相之间有代码重用和代码共享
- 通常是不开源的，无法直接获取源码

这就为代码检查与漏洞分析增加了很大的困难，因此二进制代码分析的重要性可见一斑

### 1.2 Insight

二进制代码经过反汇编后得到对应的汇编代码，这促使研究者想到NLP技术

二进制代码分析与自然语言处理有许多共同之处：

- 语义提取
- 文本片段比较
- 文本分类

因此可以把NLP与二进制代码分析结合起来

### 1.3 Previous Work

先前的工作解决了比较两份代码是否相同的问题（等效问题），而一份代码是否包含在另一份代码中的问题（内含问题）并没有有效解决

先前的工作只解决了单一处理器架构下的内含问题，而针对跨架构下的内含问题没有有效解决

### 1.4 Problems Definition

1. **Problem I: 给定一对来自不同处理器架构的二进制代码基本块，判断它们的语义是否相似**
2. **Problem II: 给定一对来自不同处理器架构的二进制代码片段，判断其中一个片段是否内含于另一个片段**

其中问题1已经有很多最近的工作正在做，但是问题2还是非常新的

#### Problem I

现存的方法无法兼顾效率与准确性

- 模糊方法(*Fuzzing*)，需要大量的运行时间
- 特征表示，不太精确

注意到二进制代码经过反汇编后得到汇编代码，在不同的处理器架构下，得到的汇编代码不同，这和比较两种不同的人类自然语言文本有相同之处，这促使研究者想到把神经机器翻译技术应用于此，用于解决不同语言的文本比较处理

通过把每条汇编指令看作自然语言文本的单词，把每个基本块看作语句，能够发现这和比较不同的自然语言是否相同有很大相似之处

研究者提出了一个全新的神经网路的模型**cross-(assembly)-lingual basic-block embedding model**，用于把基本块转化成嵌入(*Embedding*)，即一个高维向量，用于编码基本块的语义、捕捉语义特征和关系，因此两个不同的基本块的相似性可以通过比较两个高维向量之间的距离得出

这种方法还要结合深度学习的方法来自动捕捉特征，研究者提出采用LSTM来自动编码基本块到一个嵌入中，这种结合了深度学习的方法能够有很高的效率和准确性

#### Problem II

单一处理器架构下的代码内含问题已经得到很好的研究了，然而跨架构的代码内含问题的解决方案目前还没有

为了解决这个问题，研究者把程序的控制流图(*Control Flow Graph*)分解成多条路径，每条路径可以看作一个基本块的序列，基于此能够应用[Problem I](INNEREYE.md#Problem%20I)中高效且准确地比较两个基本块相似性的方法，并结合最长公共子序列算法(*Longest Common Subsequence*)进行每个路径的相似性比较，这样就可以比较两份代码的相似性了

> [!warning] 注意
> 并不能把任意的一份代码看作一个语句，因为代码并不是完全按顺序执行的，所以它的控制流图并不表现为一条直线序列，只有其中特定的一条路径会被执行，而函数的路径可以在不改变语义的情况下被编译器打乱

### 1.5 Solution

基于上述，研究者实现了一个原型系统，称为**INNEREYE**，由**INNEREYE-BB**和**INNEREYE-CC**两个子系统组成，其中前者负责解决[Problem I](INNEREYE.md#Problem%20I)，而后者负责解决[Problem II](INNEREYE.md#Problem%20II)，并且经过测试与评估，测试结果已经[公开](https://nmt4binaries.github.io)

### 1.6 Contributions

1. 借鉴神经机器翻译技术解决跨处理器架构下的二进制代码相似性比较问题，通过把汇编指令看作单词和把基本块看作语句能够很好地迁移自然语言比较的方法到跨架构二进制代码的相似性比较问题上
2. 设计一个高效且准确的嵌入模型**cross-(assembly)-lingual basic-block embedding model**，它利用单词嵌入和LSTM自动捕捉汇编指令的语义和特征
3. 把[Problem I](INNEREYE.md#Problem%20I)的解决方案应用于跨架构的代码内含问题上
4. 实现一个原型系统[1.5 Solution](INNEREYE.md#1.5%20Solution)并完成评估与测试，验证其高效性和准确性
5. 这个实验成功证明从自然语言处理的角度出发能够很好地解决二进制代码分析问题

---

## 2 Related Work

### 2.1 Traditional Code Similarity Comparison

#### Mono-Architecture Solutions

- 静态抄袭检测或克隆检测：计算成本昂贵，不适合大型代码
    - 基于字符串
    - 基于token
    - 基于语法树
    - 基于程序依赖图(PDG)
- 动态胎记方法：无法扩展到跨架构和嵌入式设备
    - 基于API
    - 基于系统调用
    - 基于函数调用

#### Cross-Architecture Solutions

- 基本块的模糊相似度比较
- 基于CFG的匹配算法

计算成本高昂，无法大规模应用

### 2.2 Machine Learning-Based Code Similarity Comparison

#### Mono-Architecture Solutions

- 源代码片段的相似度比较
- 基于抽象语法树的卷积神经网络的源代码片段相似度检测
- 基于向量距离的函数相似度检测

#### Cross-Architecture Solutions

- 基于传统机器学习和深度学习的函数CFG转换为向量的相似度比较
- 基于选择性内敛技术捕捉函数语义的函数建模

---

## 3 Overview

对于一个想要用于查询的由一系列可以由CFG表示的基本块组成的代码部分$Q$，我们想要知道在一大批来自不同处理器架构的二进制代码程序中是否有和$Q$语义相似或相同的代码部分

检查代码部分语义由三个层次组成：

- 基本块
- CFG路径
- 代码部件

系统的架构由以下组成：

1. 输入：需要查询的代码片段和目标程序组
2. 前端：反汇编所有的二进制代码并构建CFG
3. 使用**neural network-based cross-lingual basic-block embedding model**生成嵌入代表一个基本块，所有的嵌入都存储在位置敏感散列数据库中以便进行高效的在线搜索
4. 使用最长公共子序列算法进行路径的语义相似性比较，其中的最长公共子序列长度用于和查询代码片段长度进行比较，二者的比率表示嵌入在目标程序中的查询路径的语义
5. 代码部分的相似性比较通过探索多个路径对用来计算相似度分数，以表明查询代码部分在目标程序中被重用的可能性
6. 输出检测结果

### 3.1 Basic Block Similarity Detection

基本块的相似度检测的关键在于测定两个来自不同处理器架构的基本块的相似性，使用的**neural network-based cross-lingual basic-block embedding model**将一对基本块作为输入，计算它们的相似性分数$s\in[0,1]$，如果二者越相似那么它们的得分越趋向于$1$，否则越趋向于$0$

为了实现这个，这个模型采用了两端分别采用LSTM的*Siamese*架构，这是一种流行的网络架构，用于在两个可比物体之间比较相似性

LSTM适用于学习序列的长范围依赖关系，两端的LSTM联合训练以适应夸指令集架构的语法差异

这个模型使用一个很大的包含很多基本块对的数据集，每对基本块都有相似性分数作为它们的标签

### 3.2 Vector Representation

> A vector representation of an instruction and a basic block is called an instruction embedding and a block embedding, respectively.

基本块嵌入模型把每个基本块转化为一个嵌入以用于比较，基本步骤为

1. 指令嵌入生成：基本块的每条指令根据指令嵌入矩阵生成指令嵌入，矩阵由神经网络学习得到
2. 基本块嵌入生成：基本块的指令嵌入被输入到一个神经网络以生成基本块嵌入
3. 基本块嵌入距离比较：得到了基本块嵌入后就通过计算它们的距离来计算相似性

### 3.3 Advantages

继承自神经机器翻译技术，这个模型能够自动选择特征而省去手动选择的过程，相比于之前先进的模型，如Genius和Gemini，它减少了重要信息的遗失，因此精确性能够很大程度提高
